{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\James\\anaconda3\\envs\\gideon\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\James\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f023eae0dfb49eeb5eeab30f08c7821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7339cab9298e44ea9ec0436b3556f180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c165b66720c47f896137fa05da0c798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190f78d9fef94ee68208743b98055873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0deafa94f8f4c9f81d7349772d94c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09f967f0a6f412dbc0eeddc41601472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a009dc48f045349449e4963caf78ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57239d35885a4b43a38fce48e7c5bcf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9109454b39384c9cbe01b1e48eb45249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532260999caf473395d47f12dca797f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27030b161f9440cb26f8d14eac5f06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS files...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This can be used for addresses that are misspelled. Add in the CFS address and\n",
    "the victim's address to use with the LLMs to determine the best course of \n",
    "action.\n",
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import faiss\n",
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Additional imports for conversational memory.\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Make sure NLTK's sentence tokenizer is available\n",
    "nltk.download('punkt')\n",
    "\n",
    "load_dotenv(\"inc/api_keys.env\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "# Initialize the model.\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=OPENAI_MODEL, temperature=0.3)\n",
    "\"\"\"\"\"\"\n",
    "# Load a pre-trained model for text embeddings\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "try:\n",
    "    print(\"Loading FAISS files...\")\n",
    "    # Load index from disk\n",
    "    index = faiss.read_index(\"resources/canton_addresses.faiss\")\n",
    "\n",
    "    # Load the original texts from the JSON file\n",
    "    with open(\"resources/text_mapping.json\", \"r\") as f:\n",
    "        text_data = {int(k): v for k, v in json.load(f).items()}\n",
    "except: \n",
    "    print(\"Files not located.\\n Creating FAISS files...\")\n",
    "    # Prep and train model for only Canton addresses\n",
    "    df_addresses = pd.read_excel(\"resources/lib_address.xlsx\")\n",
    "    address = df_addresses.loc[df_addresses['city'] == \"CANTON\"]['full_address'].tolist()\n",
    "    embeddings = model.encode(address, verbose=0)\n",
    "\n",
    "    # Set the dimension to the embedding size\n",
    "    dimension = embeddings.shape[1]\n",
    "\n",
    "    # Create a FAISS index (Flat L2 distance)\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "    # Add the embeddings to the FAISS index\n",
    "    index.add(np.array(embeddings).astype('float32'))\n",
    "\n",
    "    # Store the original text\n",
    "    text_data = {i: text for i, text in enumerate(address)}\n",
    "\n",
    "    # Save index to disk\n",
    "    faiss.write_index(index, \"resources/canton_addresses.faiss\")\n",
    "\n",
    "    # Save the original texts in a JSON file\n",
    "    with open(\"resources/text_mapping.json\", \"w\") as f:\n",
    "        json.dump(text_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def address_suggestion(address, model, llm, k=3, vic_address=None):\n",
    "\n",
    "    # Vectorize the query text\n",
    "    query_vector = model.encode([address])\n",
    "\n",
    "    # Perform the search in FAISS (k=1 means the closest match)\n",
    "    distances, indices = index.search(np.array(query_vector).astype('float32'), k)\n",
    "\n",
    "    # Get the list of the closest matches\n",
    "    string = \"\"\n",
    "    for i in range(0,k,1):\n",
    "        string += f\"{text_data[indices[0][i]]}, \"\n",
    "    string = string[:-2]\n",
    "\n",
    "    if vic_address is not None:\n",
    "        prompt = f\"\"\"Given an address'{address}' with the person's address \n",
    "        listed as '{vic_address}' which may or may not be incorrect, compare it \n",
    "        to the list of addresses '{string}' and identify the closest match \n",
    "        based on the street name, directional quadrant, house number, and the \n",
    "        person's listed address. Prioritize matches where the house number is \n",
    "        closest, and consider slight data entry errors such as extra digits or \n",
    "        misnumbering. Then return what you believe to be the correct address \n",
    "        without explaination.If there are no matches to the list provided, reply with \n",
    "        'Needs to be added'.\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Given an address'{address}', compare it \n",
    "        to the list of addresses '{string}' and identify the closest match \n",
    "        based on the street name, directional quadrant, and house number. \n",
    "        Prioritize matches where the house number is closest, and consider \n",
    "        slight data entry errors such as extra digits or incorrect directional \n",
    "        quadrant. Then return what you believe to be the correct address \n",
    "        without explaination. If there are no matches to the list provided, reply with \n",
    "        'Needs to be added'.\"\"\"\n",
    "    \n",
    "    result = llm.invoke(prompt)\n",
    "    return result.content\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609 ARLINGTON AVE NW\n"
     ]
    }
   ],
   "source": [
    "# Correcting an address typed by the officer by using the victim's address as a close match type.\n",
    "address = '60189 Arlington AVE NE'\n",
    "vic_address = '609 Arlington Ave NW'\n",
    "result = address_suggestion(address, model, llm, k=3, vic_address=vic_address)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1401 31ST ST NE\n"
     ]
    }
   ],
   "source": [
    "# Correcting an address that was misspelled.\n",
    "address = '1401 31STT RD NE'\n",
    "vic_address = None\n",
    "result = address_suggestion(address, model, llm, k=3, vic_address=vic_address)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest text: 609 ARLINGTON AVE NW\n"
     ]
    }
   ],
   "source": [
    "# Finding the closest match for an address that was entered incorrectly.\n",
    "# Example query text\n",
    "query_text = \"6018 Arlington Ave NE\"\n",
    "\n",
    "# Vectorize the query text\n",
    "query_vector = model.encode([query_text])\n",
    "\n",
    "# Perform the search in FAISS (k=1 means the closest match)\n",
    "k = 3  # You can increase k to get more matches\n",
    "distances, indices = index.search(np.array(query_vector).astype('float32'), k)\n",
    "\n",
    "# Get the index of the closest match\n",
    "closest_index = indices[0][0]\n",
    "\n",
    "# Retrieve the corresponding text\n",
    "retrieved_text = text_data[closest_index]\n",
    "print(f\"Closest text: {retrieved_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609 ARLINGTON AVE NW, 615 ARLINGTON AVE NW, 8322 ARLINGTON AVE NW\n"
     ]
    }
   ],
   "source": [
    "# This can return multiple close matches to work from as needed.\n",
    "string = \"\"\n",
    "for i in range(0,k,1):\n",
    "    string += f\"{text_data[indices[0][i]]}, \"\n",
    "string = string[:-2]\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609 ARLINGTON AVE NW\n"
     ]
    }
   ],
   "source": [
    "# Using a detailed prompt to get the LLM to determine the best match.\n",
    "prompt = f\"\"\"Given an address'{query_text}' with the person's address listed as \n",
    "'{vic_address}' which may or may not be incorrect, compare it to the list of addresses '{string}' \n",
    "and identify the closest match based on the street name, directional quadrant, \n",
    "house number, and the person's listed address. Prioritize matches where the \n",
    "house number is closest, and consider slight data entry errors such as extra \n",
    "digits or misnumbering. Then return what you believe to be the correct address without explaination.\"\"\"\n",
    "\n",
    "result = llm.invoke(prompt)\n",
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gideon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
